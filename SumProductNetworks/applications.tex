\section{Applications}

\begin{frame}{Existing Applications}
Some existing applications of SPNs:
\begin{itemize}
    \item Computer vision, e.g.~image classification, medical image processing, attend-infer-repeat.
    \item Language processing, e.g.~language modelling, bandwidth extension.
    \item Robotics, e.g.~semantic mapping.
    \item \textbf{Non-linear regression}, and many more\footnote{\scriptsize https://github.com/arranger1044/awesome-spn\#applications}
\end{itemize}
\end{frame}

\begin{frame}{Gaussian Processes}
    A Gaussian Process (GP) is a collection of random variables indexed by an arbitrary
    covariate space $\mathcal{X}$, where any finite subset is Gaussian distributed.

    A GP can be understood as a prior over functions.

    \begin{figure}
        \includegraphics[width=0.9\textwidth]{GP_Rasmussen}
        \caption{\scriptsize C. E. Rasmussen \& C. K. I. Williams, Gaussian Processes for Machine Learning, 2006.}
    \end{figure}
\end{frame}

\begin{frame}{Gaussian Processes}
A GP is uniquely specified by a \emph{mean-function} $m: \mathcal{X} \rightarrow \mathbb{R}$ and a \emph{covariance function} $k: \mathcal{X} \times \mathcal{X} \rightarrow \mathbb{R}$.

The posterior predictive distribution (used for predictions) of a GP is Gaussian, i.e.,
\begin{equation}
    p(f^* \cbar \bm f) = \Normal\left( k^T_{\xnew,\X} k^{-1}_{\X,\X} \bm f , k_{\xnew,\xnew} - k_{\xnew,\X} k^{-1}_{\X,\X}k^T_{\xnew,\X}   \right)
\end{equation}
The inversion of $k_{\X,\X}$ is usually computes using the Cholesky decomposition of $k_{\X,\X}$?

\pause
\textbf{Problem:} The Cholesky decomposition scales $\mathcal{O}(N^3)$.
\end{frame}

\iffalse
\begin{frame}{Gaussian Processes}
We can use the Cholesky decomposition. % of $k_{\X,\X} = LL^T$ and take $(L^{-1})^T L^{-1}$ instead of $k^{-1}_{\X,\X}$.

\textbf{Problem:} The Cholesky decomposition scales $\mathcal{O}(N^3)$.

Solutions:
\begin{enumerate}
  \item Approximate the posterior using a variational approximation, or
  \item Use local experts to approximate the GP or approximate the computation of predictions.
\end{enumerate}
\end{frame}
\fi

\begin{frame}{Local Experts}
\textit{Local experts to approximate the GP or approximate the computation of predictions.}

A natural way is to partition $\mathcal{X}$ into sub-sets $\mathcal{X}^{(k)} \; , k=1, \dots, K$. This is called the naive-local-experts model.
\begin{figure}
  \includegraphics[width=0.8\textwidth]{NLE}
\end{figure}
\end{frame}


\begin{frame}{Local Experts}
Existing solutions to discontinuities.
\begin{enumerate}
    \item Product-of-Experts (PoE) / Bayesian Committee Machine (BCM)
  \begin{itemize}
      \item Instead of partition $\mathcal{X}$, partition $\data$ into sub-sets $\data^{(k)}$.
    \item Use an algorithm that works only on the sub-sets.
    \item \textbf{Problem:} Not a stochastic process, results in over-conservative or over-confident estimates.
  \end{itemize}
  \item Mixture-of-Experts (MoE)
  \begin{itemize}
    \item Use a gating network to assign observations to experts instead of hard boundaries.
    \item Often intractable (due to the gating network).
  \end{itemize}
  \item Impose Continuity Constraints
  \begin{itemize}
    \item Suffers from inconsistent variances and does not scale.
  \end{itemize}
\end{enumerate}
\end{frame}

\begin{frame}{Deep Structured Mixtures of Gaussian Processes}
Why not use a large finite mixture of NLEs?
\pause

Deep Structured Mixture of GPs (DSMGP)\footnote{\scriptsize M. Trapp et al.: Deep structure mixtures of Gaussian Processes. To appear in AISTATS, 2020.}:
\begin{enumerate}
  \item A DSMGP is an SPN (large structured mixture) over Gaussian measures.
  \item DSMGPs perform exact Bayesian model averaging over a large set of NLEs.
\end{enumerate}
\end{frame}

\begin{frame}{Deep Structured Mixtures of Gaussian Processes}
Benefits of DSMGPs:
\begin{itemize}
    \item DSMGPs are a sound stochastic process.
    \item We can perform exact posterior inference in DSMGPs.
    \item DSMGPs have similar computational costs compared to other expert-based approaches.
    \item We can exploit the structure to perform Cholesky updates incrementally and share computations.
    \item DSMGPs capture predictive uncertainties consistently better.
    \item They do not suffer from severe discontinuities and can be used to model non-stationary data.
\end{itemize}

%\begin{figure}
%  \centering
%  \includegraphics[width=0.5\linewidth]{comparison}
%\end{figure}
\end{frame}

\begin{frame}{DSMGPs - Example}
    \animategraphics[loop,controls,width=\textwidth]{10}{anim-}{0}{49}
\end{frame}
