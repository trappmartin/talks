\section{Parameter Learning}

\begin{frame}{Parameter Learning in SPNs}{}
SPNs are differentiable multi-linear NNs with non-linear inputs, thus common parameter learning for NNs can be applied.

Approaches that go beyond NN techniques:
\begin{itemize}
    \item Expectation Maximisation {\scriptsize [R. Peharz et al.: On the latent variable interpretation of sum-product networks. TPAMI, 2017.]}
    \item Variational Inference {\scriptsize [H. Zhao et al.: Collapsed variational inference for sum-product networks. In ICML, 2016.]}
    \item Bayesian moment matching {\scriptsize [A. Rashwan et al.: Online and distributed Bayesian moment matching for parameter learning in sum-product networks. In AISTATS, 2016.]}
    \item \textbf{Safe Semi-Supervised Learning} {\scriptsize [M. Trapp et al.: Safe semi-supervised learning of sum-product networks. In UAI, 2017.]}
\end{itemize}

%\begin{columns}
%\begin{column}{\linewidth}
%\begin{block}{Generative Learning\footnote{\scriptsize H. Poon \& P. Domingos: Sum-product networks: A new deep architecture. In UAI, 2011.}}
%\begin{equation}
%  \mathcal{L}(\theta \,| \, \mathcal{X}) = \sum_{n=1}^N \log \f - \log \fz\, , \; \; \xn \in \mathbb{R}^D
%\end{equation}
%\end{block}
%Note that $\fz$ is the partition function which can be evaluated efficiently using a single upward pass.
%%\hline
%\begin{block}{Discriminative Learning\footnote{\scriptsize R. Gens \& P. Domingos: Discriminative learning of sum-product networks. In NeurIPS, 2012.}}
%\begin{equation}
%  \begin{aligned}
%    \mathcal{L}(\theta, \lambda \,| \, \mathcal{X}) = \sum_{n=1}^N &\log \fy - \log \f\, , \; \; \xn \in \mathbb{R}^D, \, \lambda_n \in \mathbb{R}
%\end{aligned}
%\end{equation}
%\end{block}
%\end{column}
%\end{columns}
\end{frame}

\begin{frame}{Safe Semi-Supervised Learning}{}
    \begin{block}{Contrastive Pessimistic Likelihood Estimation (CPLE)}
\begin{itemize}
    \item Assume $\mathcal{X} = \{\bm x_n, \lambda_n\}_{n=1}^N$, $\lambda_n \in \mathcal{R}^K$ to be a small set of labelled data points and $\mathcal{U} = \{\bm u_m\}_{m=1}^M$ a set of unlabelled data points.
    \item $\theta^+$ is an SPN trained solely on $\mathcal{X}$
    \item We propose soft labels $\bm q_m \in \Delta_{K-1}$ for all unlabelled data points, e.g.~randomly.
    \item We initialise the semi-supervised SPN $\theta$ randomly, then:
    \pause
    \begin{enumerate}
        \item Update all soft labels so that the performance of $\theta^+$ on all data points is better than $\theta$.
        \item Update $\theta$ so that the performance of $\theta$ on all data points is better than $\theta^+$.
        \item If not converged, move to step 1.
    \end{enumerate}
\end{itemize}

\end{block}
\end{frame}

%\begin{equation}
%  \text{CPLE} = \argmax_{\theta \in \Theta} \argmin_{\bm q \in \Delta_{K-1}^M} \mathcal{L}(\theta,  \lambda, \bm{q} \,| \, \mathcal{X}, \mathcal{U}) - \mathcal{L}(\theta^+, \lambda, \bm{q} \,| \, \mathcal{X}, \mathcal{U})
%\end{equation}

\begin{frame}{Semi-Supervised Learning}{}
\begin{figure}
    \includegraphics[width=.47\textwidth]{semisupervised_2_2}~
    \includegraphics[width=.47\textwidth]{semisupervised_20_2}
    \caption{Decision boundary of a semi-supervised SPN at iteration 1 (left) and after convergence (right).}
\end{figure}

\end{frame}

\begin{frame}{Over-parametrization in SPNs\footnote{\scriptsize M. Trapp et al.: Optimisation of Overparametrized Sum-Product Networks. ICML Workshop on Tractable Probabilistic Models, 2019.}}{}
\begin{columns}
\begin{column}{\linewidth}
\onslide<1->{

  \begin{columns}
  \begin{column}{0.5\linewidth}
    \includegraphics[width=\textwidth]{shallowSPN}
  \end{column}
  \begin{column}{0.5\linewidth}
    \includegraphics[width=\textwidth]{deepSPN}
  \end{column}
  \end{columns}
}
\onslide<2->{
\begin{align}
    \wt &\approx \wt + {\color{cyan} \rho^{(t)}} \nabla_{\wt} + {\color{blue}\sum_{\tau=1}^{t-1} \mu^{(t,\tau)} \nabla_{w^{(\tau)}_k}}
\end{align}\\
%  \begin{tcolorbox}[lower separated=false]
%  \small{Gradient-based optimisation in deep tree-structured sum-product network with small (fixed) learning rate and near zero initialisation of the weights is equivalent to gradient-based optimisation with adaptive and time-varying {\color{cyan} learning rate} and {\color{blue} momentum term}.}
% \end{tcolorbox}
}
\end{column}
\end{columns}
\end{frame}

\begin{frame}{Over-parametrization Experiment}
\begin{columns}
\begin{column}{\linewidth}
\includegraphics[width=\textwidth]{nltcs_experiment}
\end{column}
\end{columns}
\end{frame}

\iffalse
\begin{frame}{Bayesian Parameter Learning}
  \begin{itemize}
    \item The key insight for Bayesian parameter learning\footnote{\scriptsize Zhao et al.: Collapsed variational inference for sum-product networks. In ICML, 2016.}
        is that \emph{sum nodes} can be interpreted as \emph{latent variables} $Z_\SumNode$, clustering data instances.
    \item Given a vector of states for each sum, $\z$ induces a so-called induced tree ($\SPT$) on $\SPN$.
  \end{itemize}
  \begin{figure}
  \centering{
    \includestandalone[width=0.7\textwidth]{InducedTree}
    }
\end{figure}
\end{frame}
\fi

%\begin{frame}{Bayesian Parameter Learning}
%  \begin{itemize}
%    \item It is now conceptually straightforward to extend an SPN to a Bayesian setting.
%  \end{itemize}
%  \pause
%  \begin{figure}%
%\centering%
%\begin{tikzpicture}%
%  \node[obs, fill=lightbackground, draw=white] (x) {$\xnd$};%
%  \node[foreground, draw=CBYellow, latent, fill=background, left=of x] (z) {$z_{\SumNode,n}$};%
%  \node[foreground, draw=CBPurple, latent, fill=background, right=of x] (t) {$\theta_{\Leaf,d}$};%
%  \node[foreground, draw=CBGreen, latent, fill=background, below=of z] (w) {$w_{\SumNode}$};%
%%
%  \edge {z,t} {x} ; %%
%  \edge {w} {z} ; %%
%%
%  \plate {zs} {(z)(w)} {$\forall \SumNode \in \SumNodes$} ;%
%  \plate {t} {(t)} {$\forall \Leaf \in \Leaves$} ;%
%%
%  \plate [inner xsep=0.3cm] {xyt} {(x)(t)} {$d\!\in\!1\!:\!D$} ;%
%  \plate [inner xsep=0.3cm] {xz} {(x)(z)(xyt.north west)} {$n\!\in\!1\!:\!N$} ;%
%\end{tikzpicture}%
%\caption{Generative model for Bayesian parameter learning.} \label{fig:BSPN}%
%\end{figure}%
%\end{frame}
