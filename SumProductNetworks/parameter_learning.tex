%\section{Parameter Learning}

\begin{frame}{Parameter Learning in SPNs}{}
%SPNs are differentiable multi-linear NNs with non-linear inputs, thus common parameter learning for NNs can be applied.
We can use backprop for parameter learning in SPNs.

More advanced approaches:
\begin{itemize}
    \item Expectation Maximisation {\scriptsize [R. Peharz et al.: On the latent variable interpretation of sum-product networks. TPAMI, 2017.]}
    \item Variational Inference {\scriptsize [H. Zhao et al.: Collapsed variational inference for sum-product networks. In ICML, 2016.]}
    \item Bayesian moment matching {\scriptsize [A. Rashwan et al.: Online and distributed Bayesian moment matching for parameter learning in SPNs. In AISTATS, 2016.]}
    \item Safe Semi-Supervised Learning {\scriptsize [M. Trapp et al.: Safe semi-supervised learning of sum-product networks. In UAI, 2017.]}
\end{itemize}

Gradient-based optimisation in deep tree-structured SPNs has implicit acceleration effects. {\scriptsize [M. Trapp et al.: Optimisation of overparametrized sum-product networks. ICML Workshop on TPM, 2019.]}
\end{frame}

\iffalse
\begin{frame}{Safe Semi-Supervised Learning}{}
    \begin{block}{Contrastive Pessimistic Likelihood Estimation (CPLE)}
        \begin{itemize}
            \item $\mathcal{X} = \{\bm x_n, \lambda_n\}_{n=1}^N$, $\lambda_n \in \mathcal{R}^K$  = labelled data points
            \item $\mathcal{U} = \{\bm u_m\}_{m=1}^M$ = unlabelled data points
            \item $\bm q_m \in \Delta_{K-1}$ = soft labels
            \item $\theta^+$ = SPN trained only on $\mathcal{X}$
            \item $\theta$ = SPN trained using CPLE on all data points
            \item $\mathcal{L}(\theta | \mathcal{X}, \mathcal{U}, \bm q)$ = likelihood or conditional likelihood
        \end{itemize}

    \pause
    Training:
    \begin{enumerate}
        \item Update soft labels so that $\mathcal{L}(\theta^+ | \mathcal{X}, \mathcal{U}, \bm q) > \mathcal{L}(\theta | \mathcal{X}, \mathcal{U}, \bm q)$.
        \item Update $\theta$ so that $\mathcal{L}(\theta^+ | \mathcal{X}, \mathcal{U}, \bm q) < \mathcal{L}(\theta | \mathcal{X}, \mathcal{U}, \bm q)$.
        \item If not converged, move to step 1.
    \end{enumerate}
\end{block}
\end{frame}
\fi

\begin{frame}{Safe Semi-Supervised Learning - Example}{}
\begin{figure}
    \includegraphics[width=.47\textwidth]{semisupervised_2_2}~
    \includegraphics[width=.47\textwidth]{semisupervised_20_2}
    \caption{Decision boundary of a semi-supervised SPN at iteration 1 (left) and after convergence (right).}
\end{figure}
\end{frame}

\iffalse
\begin{frame}{Over-parametrization in SPNs\footnote{\scriptsize M. Trapp et al.: Optimisation of Overparametrized Sum-Product Networks. ICML Workshop on Tractable Probabilistic Models, 2019.}}{}
\begin{columns}
\begin{column}{\linewidth}
\onslide<1->{

  \begin{columns}
  \begin{column}{0.5\linewidth}
    \includegraphics[width=\textwidth]{shallowSPN}
  \end{column}
  \begin{column}{0.5\linewidth}
    \includegraphics[width=\textwidth]{deepSPN}
  \end{column}
  \end{columns}
}
\onslide<2->{
\begin{align}
    \wt &\approx \wt + {\color{cyan} \rho^{(t)}} \nabla_{\wt} + {\color{blue}\sum_{\tau=1}^{t-1} \mu^{(t,\tau)} \nabla_{w^{(\tau)}_k}}
\end{align}\\
}
\end{column}
\end{columns}
\end{frame}
\fi

\iffalse
\begin{frame}{Over-parametrization - Experiment}
\begin{columns}
\begin{column}{\linewidth}
\includegraphics[width=\textwidth]{nltcs_experiment}
\end{column}
\end{columns}
\end{frame}

\begin{frame}{Bayesian Parameter Learning}
  \begin{itemize}
    \item The key insight for Bayesian parameter learning\footnote{\scriptsize Zhao et al.: Collapsed variational inference for sum-product networks. In ICML, 2016.}
        is that \emph{sum nodes} can be interpreted as \emph{latent variables} $Z_\SumNode$, clustering data instances.
    \item Given a vector of states for each sum, $\z$ induces a so-called induced tree ($\SPT$) on $\SPN$.
  \end{itemize}
  \begin{figure}
  \centering{
    \includestandalone[width=0.7\textwidth]{InducedTree}
    }
\end{figure}
\end{frame}
\fi

%\begin{frame}{Bayesian Parameter Learning}
%  \begin{itemize}
%    \item It is now conceptually straightforward to extend an SPN to a Bayesian setting.
%  \end{itemize}
%  \pause
%  \begin{figure}%
%\centering%
%\begin{tikzpicture}%
%  \node[obs, fill=lightbackground, draw=white] (x) {$\xnd$};%
%  \node[foreground, draw=CBYellow, latent, fill=background, left=of x] (z) {$z_{\SumNode,n}$};%
%  \node[foreground, draw=CBPurple, latent, fill=background, right=of x] (t) {$\theta_{\Leaf,d}$};%
%  \node[foreground, draw=CBGreen, latent, fill=background, below=of z] (w) {$w_{\SumNode}$};%
%%
%  \edge {z,t} {x} ; %%
%  \edge {w} {z} ; %%
%%
%  \plate {zs} {(z)(w)} {$\forall \SumNode \in \SumNodes$} ;%
%  \plate {t} {(t)} {$\forall \Leaf \in \Leaves$} ;%
%%
%  \plate [inner xsep=0.3cm] {xyt} {(x)(t)} {$d\!\in\!1\!:\!D$} ;%
%  \plate [inner xsep=0.3cm] {xz} {(x)(z)(xyt.north west)} {$n\!\in\!1\!:\!N$} ;%
%\end{tikzpicture}%
%\caption{Generative model for Bayesian parameter learning.} \label{fig:BSPN}%
%\end{figure}%
%\end{frame}
